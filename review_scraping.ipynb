{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a483f45",
   "metadata": {},
   "source": [
    "Scraping Data from Web"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94811323",
   "metadata": {},
   "source": [
    "Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d537e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests\n",
      "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests)\n",
      "  Downloading charset_normalizer-3.4.3-cp313-cp313-win_amd64.whl.metadata (37 kB)\n",
      "Collecting idna<4,>=2.5 (from requests)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests)\n",
      "  Downloading urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests)\n",
      "  Downloading certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)\n",
      "Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Downloading charset_normalizer-3.4.3-cp313-cp313-win_amd64.whl (107 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Downloading certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
      "Installing collected packages: urllib3, python-dotenv, idna, charset_normalizer, certifi, requests\n",
      "\n",
      "   ---------------------------------------- 0/6 [urllib3]\n",
      "   ------------- -------------------------- 2/6 [idna]\n",
      "   -------------------- ------------------- 3/6 [charset_normalizer]\n",
      "   --------------------------------- ------ 5/6 [requests]\n",
      "   ---------------------------------------- 6/6 [requests]\n",
      "\n",
      "Successfully installed certifi-2025.8.3 charset_normalizer-3.4.3 idna-3.10 python-dotenv-1.1.1 requests-2.32.5 urllib3-2.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82b0430",
   "metadata": {},
   "source": [
    "Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e138572",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, csv, random, sys\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from dotenv import load_dotenv\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717e8f5d",
   "metadata": {},
   "source": [
    "Load API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4359cac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "API_KEY = os.environ.get(\"PLACES_API_KEY\")\n",
    "if not API_KEY:\n",
    "    raise RuntimeError(\"ERROR: PLACES_API_KEY not found in .env\")\n",
    "\n",
    "TEXT_SEARCH_URL   = \"https://places.googleapis.com/v1/places:searchText\"\n",
    "PLACE_DETAILS_URL = \"https://places.googleapis.com/v1/places/{place_id}\"\n",
    "\n",
    "SESSION = requests.Session()\n",
    "SESSION.headers.update({\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"X-Goog-Api-Key\": API_KEY,\n",
    "})\n",
    "\n",
    "def backoff_sleep(attempt):\n",
    "    time.sleep(min(2 ** attempt + random.random(), 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ecf444d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SG_RECT = {\n",
    "    \"low_lat\": 1.130, \"low_lng\": 103.600,\n",
    "    \"high_lat\": 1.475, \"high_lng\": 104.100\n",
    "}\n",
    "\n",
    "QUERIES = [\n",
    "    \"hawker centre\", \"food court\", \"coffee shop\", \"bar\", \"nightclub\", \"karaoke\",\n",
    "    \"massage\", \"mobile phone repair\", \"electronics store\", \"budget hotel\", \"hostel\",\n",
    "    \"car workshop\", \"clinic\", \"pawn shop\", \"arcade\", \"supermarket\", \"convenience store\",\n",
    "    \"tourist attraction\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a26bf343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_search_batch(query, rect, page_size=20, max_pages=3):\n",
    "    places = []\n",
    "    body = {\n",
    "        \"textQuery\": query,\n",
    "        \"locationRestriction\": {\n",
    "            \"rectangle\": {\n",
    "                \"low\": {\"latitude\": rect[\"low_lat\"], \"longitude\": rect[\"low_lng\"]},\n",
    "                \"high\": {\"latitude\": rect[\"high_lat\"], \"longitude\": rect[\"high_lng\"]},\n",
    "            }\n",
    "        },\n",
    "        \"pageSize\": page_size\n",
    "    }\n",
    "    headers = {\"X-Goog-FieldMask\": \"places.id,places.displayName,nextPageToken\"}\n",
    "\n",
    "    next_token = None\n",
    "    for _ in range(max_pages):\n",
    "        payload = dict(body)\n",
    "        if next_token:\n",
    "            payload[\"pageToken\"] = next_token\n",
    "        r = SESSION.post(TEXT_SEARCH_URL, headers=headers, json=payload)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        places.extend(data.get(\"places\", []) or [])\n",
    "        next_token = data.get(\"nextPageToken\")\n",
    "        if not next_token:\n",
    "            break\n",
    "    return places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d42659e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def place_details_with_reviews(place_id, verbose=False):\n",
    "    \"\"\"\n",
    "    Fetch Place Details (New) with reviews + first photo URL.\n",
    "    Returns (meta, reviews_list).\n",
    "    \"\"\"\n",
    "    field_mask = \",\".join([\n",
    "        \"id\",\"displayName\",\"googleMapsUri\",\"types\",\"location\",\n",
    "        \"rating\",\"userRatingCount\",\"reviews\",\"photos\"  # <-- photos added\n",
    "    ])\n",
    "    headers = {\"X-Goog-FieldMask\": field_mask}\n",
    "    url = PLACE_DETAILS_URL.format(place_id=place_id)\n",
    "\n",
    "    r = SESSION.get(url, headers=headers, timeout=30)\n",
    "    if r.status_code != 200:\n",
    "        if verbose:\n",
    "            print(f\"[Details] ERROR for {place_id} â€” {r.status_code} {r.text[:200]}\")\n",
    "        return None, []\n",
    "\n",
    "    data = r.json()\n",
    "\n",
    "    # Build a URL for the *first* photo if present\n",
    "    photo_url = None\n",
    "    photos = data.get(\"photos\") or []\n",
    "    if photos:\n",
    "        ref = photos[0].get(\"name\")  # photo resource name\n",
    "        if ref:\n",
    "            # New Places Photo API style\n",
    "            photo_url = f\"https://places.googleapis.com/v1/{ref}/media?maxHeightPx=400&key={API_KEY}\"\n",
    "\n",
    "    meta = {\n",
    "        \"place_id\": data.get(\"id\"),\n",
    "        \"place_name\": (data.get(\"displayName\") or {}).get(\"text\"),\n",
    "        \"gmaps_url\": data.get(\"googleMapsUri\"),\n",
    "        \"types\": \",\".join(data.get(\"types\", [])) if data.get(\"types\") else None,\n",
    "        \"place_rating\": data.get(\"rating\"),\n",
    "        \"place_user_rating_count\": data.get(\"userRatingCount\"),\n",
    "        \"lat\": (data.get(\"location\") or {}).get(\"latitude\"),\n",
    "        \"lng\": (data.get(\"location\") or {}).get(\"longitude\"),\n",
    "        \"photo_url\": photo_url,  # <-- new column\n",
    "    }\n",
    "    reviews = data.get(\"reviews\", []) or []\n",
    "    return meta, reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "988ee09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def harvest_reviews(\n",
    "    queries,\n",
    "    rect,\n",
    "    target_reviews=5000,          # total rows desired in the CSV (existing + new)\n",
    "    per_place_cap=5,\n",
    "    oversample_places=2000,\n",
    "    csv_name=\"google_reviews_singapore.csv\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Resume-safe:\n",
    "      - If csv exists, append to it and skip duplicates (place_id + publish_time).\n",
    "      - Writes 'photo_url' (first photo) per place.\n",
    "      - Returns (csv_name, total_rows_written_in_file).\n",
    "    \"\"\"\n",
    "    # --- load existing rows (if any) to de-dup ---\n",
    "    existing_pairs = set()\n",
    "    total_existing = 0\n",
    "    if os.path.exists(csv_name):\n",
    "        with open(csv_name, \"r\", encoding=\"utf-8\") as f:\n",
    "            r = csv.DictReader(f)\n",
    "            for row in r:\n",
    "                total_existing += 1\n",
    "                pid = row.get(\"place_id\")\n",
    "                pub = row.get(\"publish_time\")\n",
    "                if pid and pub:\n",
    "                    existing_pairs.add((pid, pub))\n",
    "        print(f\"[Resume] Found existing file with {total_existing} rows\")\n",
    "\n",
    "    # --- collect candidate place IDs via Text Search ---\n",
    "    all_places = []\n",
    "    for q in queries:\n",
    "        batch = text_search_batch(q, rect, page_size=20, max_pages=3)\n",
    "        all_places.extend(batch)\n",
    "\n",
    "    ids, seen = [], set()\n",
    "    for p in all_places:\n",
    "        pid = p.get(\"id\")\n",
    "        if pid and pid not in seen:\n",
    "            seen.add(pid)\n",
    "            ids.append(pid)\n",
    "    random.shuffle(ids)\n",
    "    if oversample_places:\n",
    "        ids = ids[:oversample_places]\n",
    "    print(f\"[Harvest] Candidate place_ids to inspect: {len(ids)}\")\n",
    "\n",
    "    # --- open CSV for append ---\n",
    "    fieldnames = [\n",
    "        \"place_id\",\"place_name\",\"gmaps_url\",\"types\",\"place_rating\",\"place_user_rating_count\",\n",
    "        \"lat\",\"lng\",\"photo_url\",               # <-- photo_url included here\n",
    "        \"review_rating\",\"review_text\",\"review_language\",\n",
    "        \"relative_time\",\"publish_time\",\"author\"\n",
    "    ]\n",
    "    file_exists = os.path.exists(csv_name)\n",
    "    f = open(csv_name, \"a\", newline=\"\", encoding=\"utf-8\")\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    if not file_exists or total_existing == 0:\n",
    "        writer.writeheader()\n",
    "\n",
    "    total_rows = total_existing\n",
    "    try:\n",
    "        for i, pid in enumerate(ids, 1):\n",
    "            meta, reviews = place_details_with_reviews(pid, verbose=False)\n",
    "            if not meta:\n",
    "                continue\n",
    "\n",
    "            kept_here = 0\n",
    "            for rv in reviews[:per_place_cap]:\n",
    "                pair = (meta[\"place_id\"], rv.get(\"publishTime\"))\n",
    "                if pair in existing_pairs:\n",
    "                    continue  # skip duplicate (already in CSV)\n",
    "\n",
    "                row = {\n",
    "                    **meta,  # includes photo_url\n",
    "                    \"review_rating\": rv.get(\"rating\"),\n",
    "                    \"review_text\": (rv.get(\"originalText\") or {}).get(\"text\") or (rv.get(\"text\") or \"\"),\n",
    "                    \"review_language\": (rv.get(\"originalText\") or {}).get(\"languageCode\"),\n",
    "                    \"relative_time\": rv.get(\"relativePublishTimeDescription\"),\n",
    "                    \"publish_time\": rv.get(\"publishTime\"),\n",
    "                    \"author\": (rv.get(\"authorAttribution\") or {}).get(\"displayName\"),\n",
    "                }\n",
    "                writer.writerow(row)\n",
    "                existing_pairs.add(pair)\n",
    "                kept_here += 1\n",
    "                total_rows += 1\n",
    "\n",
    "                if total_rows >= target_reviews:\n",
    "                    print(f\"[Harvest] Reached target {target_reviews}. Wrote: {csv_name}\")\n",
    "                    return csv_name, total_rows\n",
    "\n",
    "            if i % 25 == 0:\n",
    "                print(f\"[Harvest] Processed {i} places â€” CSV now has {total_rows} rows\")\n",
    "\n",
    "        print(f\"[Harvest] Finished candidates. CSV has {total_rows} rows. Wrote: {csv_name}\")\n",
    "        return csv_name, total_rows\n",
    "    finally:\n",
    "        f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0c11f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Harvest] Candidate place_ids to inspect: 1055\n",
      "[Harvest] Processed 25 places â€” CSV now has 120 rows\n",
      "[Harvest] Processed 50 places â€” CSV now has 241 rows\n",
      "[Harvest] Processed 75 places â€” CSV now has 358 rows\n",
      "[Harvest] Processed 100 places â€” CSV now has 468 rows\n",
      "[Harvest] Processed 125 places â€” CSV now has 584 rows\n",
      "[Harvest] Processed 150 places â€” CSV now has 701 rows\n",
      "[Harvest] Processed 175 places â€” CSV now has 811 rows\n",
      "[Harvest] Processed 200 places â€” CSV now has 927 rows\n",
      "[Harvest] Processed 225 places â€” CSV now has 1051 rows\n",
      "[Harvest] Processed 250 places â€” CSV now has 1167 rows\n",
      "[Harvest] Processed 275 places â€” CSV now has 1285 rows\n",
      "[Harvest] Processed 300 places â€” CSV now has 1400 rows\n",
      "[Harvest] Processed 325 places â€” CSV now has 1516 rows\n",
      "[Harvest] Processed 350 places â€” CSV now has 1636 rows\n",
      "[Harvest] Processed 375 places â€” CSV now has 1755 rows\n",
      "[Harvest] Processed 400 places â€” CSV now has 1877 rows\n",
      "[Harvest] Processed 425 places â€” CSV now has 1991 rows\n",
      "[Harvest] Processed 450 places â€” CSV now has 2106 rows\n",
      "[Harvest] Processed 475 places â€” CSV now has 2212 rows\n",
      "[Harvest] Processed 500 places â€” CSV now has 2336 rows\n",
      "[Harvest] Processed 525 places â€” CSV now has 2456 rows\n",
      "[Harvest] Processed 550 places â€” CSV now has 2571 rows\n",
      "[Harvest] Processed 575 places â€” CSV now has 2686 rows\n",
      "[Harvest] Processed 600 places â€” CSV now has 2798 rows\n",
      "[Harvest] Processed 625 places â€” CSV now has 2923 rows\n",
      "[Harvest] Processed 750 places â€” CSV now has 3043 rows\n",
      "[Harvest] Processed 775 places â€” CSV now has 3100 rows\n",
      "[Harvest] Finished candidates. CSV has 3120 rows. Wrote: google_reviews_singapore.csv\n",
      "Result: google_reviews_singapore.csv\n"
     ]
    }
   ],
   "source": [
    "csv_path, n = harvest_reviews(\n",
    "    QUERIES,                 # your list of text queries\n",
    "    SG_RECT,            # your bounding box\n",
    "    target_reviews=5000,     # total rows you want in the file (existing + new)\n",
    "    per_place_cap=5,         # API returns up to 5 reviews/place\n",
    "    oversample_places=2500,  # more IDs â†’ more chances to reach target\n",
    "    csv_name=\"google_reviews_singapore.csv\"\n",
    ")\n",
    "print(\"Result:\", csv_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "techjam2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
