{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6677d8c5",
   "metadata": {},
   "source": [
    "# Labelling with OpenAI GPT 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097a6232",
   "metadata": {},
   "source": [
    "As dataset is mostly unlabelled with our intended use, we decided to call OpenAI API to label the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c24972",
   "metadata": {},
   "source": [
    "## Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e706d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai python-dotenv pandas tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d065ce6",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cb293a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import gzip\n",
    "import tiktoken\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ab862d",
   "metadata": {},
   "source": [
    "## Login to OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25e6d24",
   "metadata": {},
   "source": [
    "Place OpenAI API key in `.env` file as mentioned in `README.md`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10802b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = OpenAI(api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81269ea",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f256f20",
   "metadata": {},
   "source": [
    "We load data either from kaggle or UCSD dataset as mentioned in `README.md`. Data without text review is dropped as text review is the main source of good review as explained in `README.md`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba9b524",
   "metadata": {},
   "source": [
    "### Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5799f4",
   "metadata": {},
   "source": [
    "This dataset is from kaggle. Download the dataset as mentioned in `README.md` and place in `./data` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21b59f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/reviews.csv\")\n",
    "df = df.dropna(subset=[\"text\"]).reset_index(drop=True)\n",
    "print(len(df))\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf6ad3b",
   "metadata": {},
   "source": [
    "### UCSD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ae367e",
   "metadata": {},
   "source": [
    "This dataset is from UCSD. Download the dataset as mentioned in `README.md` and place in `./data` directory. We chose to use Vermont 10-core datasest as it is smallest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef43405",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(path):\n",
    "    with gzip.open(path, 'rt', encoding='utf-8') as g:\n",
    "        for line in g:\n",
    "            yield json.loads(line)\n",
    "data = list(parse(\"./data/review-Vermont_10.json.gz\"))\n",
    "df = pd.DataFrame(data)\n",
    "df = df.dropna(subset=[\"text\"]).reset_index(drop=True)\n",
    "print(len(df))\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6704499b",
   "metadata": {},
   "source": [
    "## Creating prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7dc0b6",
   "metadata": {},
   "source": [
    "We created the system prompt to make OpenAI API respond with the most accurate label. The prompt is to classify the review into the 3 policy violations and clean classes based purely on review text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb7a8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT_STRING = \"\"\"You are a strict moderation judge for location reviews. Classify each review into exactly ONE of:\n",
    "advertisement (self-promo, discount codes, contact links),\n",
    "irrelevant (off-topic, questions/chat unrelated to a real visit),\n",
    "rant_without_visit (angry/defamatory without evidence of an actual visit),\n",
    "clean (a normal on-topic review—positive or negative—from a real/likely visit).\n",
    "If multiple seem plausible, choose the most severe (advertisement > irrelevant > rant_without_visit > clean).\n",
    "Ignore emojis, casing, punctuation spam, and translation artifacts.\n",
    "Do not guess facts.\"\"\"\n",
    "\n",
    "df[\"prompt\"] = df[\"text\"].apply(lambda review: f\"Classify this review:\\n<review>{review}</review>\")\n",
    "print(df[\"prompt\"].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be78aa6c",
   "metadata": {},
   "source": [
    "## Create batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afb63ad",
   "metadata": {},
   "source": [
    "Due to enqueue token limit from OpenAI API, we have to set to 900k for gpt-5 and 200k for gpt-5-nano and gpt-5-mini. We estimate the token and distribute the prompts into batches before calling OpenAI batch API. Batch API can process faster than individual call and have a higher daily limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a495cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.encoding_for_model(\"gpt-5-nano\")\n",
    "\n",
    "def count_tokens(text):\n",
    "    return len(enc.encode(text))\n",
    "\n",
    "MAX_TOKENS = 900000\n",
    "batches = []\n",
    "current_batch = []\n",
    "current_tokens = 0\n",
    "\n",
    "for idx, prompt in enumerate(df[\"prompt\"], start=1):\n",
    "    prompt_tokens = count_tokens(prompt) + count_tokens(SYSTEM_PROMPT_STRING) + 100\n",
    "    if current_tokens + prompt_tokens > MAX_TOKENS:\n",
    "        batches.append(current_batch)\n",
    "        current_batch = []\n",
    "        current_tokens = 0\n",
    "    current_batch.append((idx, prompt))\n",
    "    current_tokens += prompt_tokens\n",
    "\n",
    "if current_batch:\n",
    "    batches.append(current_batch)\n",
    "    \n",
    "print(len(batches))\n",
    "print(batches[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e86200",
   "metadata": {},
   "source": [
    "## Labelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c93e387",
   "metadata": {},
   "source": [
    "Due to daily limit from OpenAI, we have used gpt-5-nano, gpt-5-mini and gpt-5 (in this order) for labelling task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45faf5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonl_file = \"./data/requests.jsonl\"\n",
    "responses_file = \"./data/responses.jsonl\"\n",
    "if os.path.exists(jsonl_file):\n",
    "    os.remove(jsonl_file)\n",
    "if os.path.exists(responses_file):\n",
    "    os.remove(responses_file)\n",
    "for i, batch_prompts in enumerate(batches):\n",
    "    with open(jsonl_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for idx, prompt in batch_prompts:\n",
    "            request_obj = {\n",
    "                \"custom_id\": f\"{idx}\",\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                \"model\": \"gpt-5\",\n",
    "                \"messages\": [\n",
    "                        {\"role\":\"system\",\"content\": SYSTEM_PROMPT_STRING},\n",
    "                        {\"role\":\"user\",\"content\": prompt}\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "            f.write(json.dumps(request_obj) + \"\\n\")\n",
    "    batch_input_file = client.files.create(\n",
    "        file=open(jsonl_file, \"rb\"),\n",
    "        purpose=\"batch\"\n",
    "    )\n",
    "    batch_input_file_id = batch_input_file.id\n",
    "    batch = client.batches.create(\n",
    "        input_file_id=batch_input_file_id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=\"24h\",\n",
    "        metadata={\n",
    "            \"description\": \"nightly eval job\"\n",
    "        }\n",
    "    )\n",
    "    batch_id = batch.id\n",
    "    batch = client.batches.retrieve(batch_id)\n",
    "    while (batch.status != \"completed\"):\n",
    "        if (batch.status == \"failed\"):\n",
    "            raise Exception(\"Batch failed\")\n",
    "        time.sleep(5)\n",
    "        batch = client.batches.retrieve(batch_id)\n",
    "    file_response = client.files.content(batch.output_file_id)\n",
    "    with open(responses_file, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(file_response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d12b04c",
   "metadata": {},
   "source": [
    "## Save to csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca8b8cc",
   "metadata": {},
   "source": [
    "We read in the responses from OpenAI API and add the label to the original dataset. `k` can be set to add the label starting from row k. Other rows will be `NaN` value if not given any labels. The final files is save to `./data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61de82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "df_responses = pd.read_json(responses_file, lines=True)\n",
    "df[\"label\"] = np.nan\n",
    "labels = df_responses[\"response\"].apply(lambda x: x['body']['choices'][0]['message']['content'])\n",
    "df.loc[k : k + len(labels) - 1, \"label\"] = labels.values\n",
    "df.to_csv(\"./data/review-Vermont_10-labeled.csv\", index=False)\n",
    "print(df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "techjam2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
