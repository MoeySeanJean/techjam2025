{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87dd6886",
   "metadata": {},
   "source": [
    "# BERT testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0712600",
   "metadata": {},
   "source": [
    "This is for testing our BERT model on Kaggle and manually scraped datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3248d22",
   "metadata": {},
   "source": [
    "## Install packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188e679b",
   "metadata": {},
   "source": [
    "Download torch according to official website and CUDA version as mentioned in `README.md`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6657f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers datasets pandas scikit-learn googletrans\n",
    "%pip install torch torchvision --index-url https://download.pytorch.org/whl/cu129"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ff4612",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46998e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from googletrans import Translator\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b0c699",
   "metadata": {},
   "source": [
    "## Load trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d014ae9e",
   "metadata": {},
   "source": [
    "This is to load the trained model from training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4aa3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"./data/saved_model\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BertForSequenceClassification.from_pretrained(save_dir, local_files_only=True)\n",
    "model.to(device)\n",
    "tokenizer = BertTokenizer.from_pretrained(save_dir, local_files_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e22c5b",
   "metadata": {},
   "source": [
    "## Load testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc49aa23",
   "metadata": {},
   "source": [
    "For testing and evaluation of our model, we used 2 datasets. One is Kaggle dataset as mentioned in `README.md`. The other is scrapped and labelled manually from local (Singapore) Google Maps reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac5c691",
   "metadata": {},
   "source": [
    "### Kaggle data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2e76d9",
   "metadata": {},
   "source": [
    "This data is from kaggle (see `README.md`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0249e81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/reviews-labeled.csv\")\n",
    "df = df[['text', 'label']]\n",
    "df = df.dropna(subset=[\"text\"]).reset_index(drop=True)\n",
    "df['label'] = df['label'].apply(lambda x : 0 if x == \"clean\" else 1)\n",
    "\n",
    "test_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"], \n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "tokenized_dataset = test_dataset.map(tokenize_fn, batched=True)\n",
    "\n",
    "tokenized_dataset.set_format(\n",
    "    type=\"torch\",\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"label\"]\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(tokenized_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f148ed",
   "metadata": {},
   "source": [
    "### Local (Singapore) data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0107063",
   "metadata": {},
   "source": [
    "This data is manually scrapped and labelled from Google Maps directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce8abe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/test-data-labeled.csv\")\n",
    "df = df[['text', 'label']]\n",
    "df = df.dropna(subset=[\"text\"]).reset_index(drop=True)\n",
    "\n",
    "texts = df['text'].to_list()\n",
    "indices = []\n",
    "texts_to_translate = []\n",
    "translated_texts = texts.copy()\n",
    "async def translate_bulk():\n",
    "    async with Translator() as translator:\n",
    "        for index, text in enumerate(texts):\n",
    "            result = await translator.detect(text)\n",
    "            if result.lang != 'en':\n",
    "                indices.append(index)\n",
    "                texts_to_translate.append(text)\n",
    "        translations = await translator.translate(texts_to_translate)\n",
    "        for i, translation in zip(indices, translations):\n",
    "            translated_texts[i] = translation.text\n",
    "await translate_bulk()\n",
    "df['text'] = translated_texts\n",
    "\n",
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        \"\\U0001F600-\\U0001F64F\"\n",
    "        \"\\U0001F300-\\U0001F5FF\"\n",
    "        \"\\U0001F680-\\U0001F6FF\"\n",
    "        \"\\U0001F700-\\U0001F77F\"\n",
    "        \"\\U0001F780-\\U0001F7FF\"\n",
    "        \"\\U0001F800-\\U0001F8FF\"\n",
    "        \"\\U0001F900-\\U0001F9FF\"\n",
    "        \"\\U0001FA00-\\U0001FA6F\"\n",
    "        \"\\U0001FA70-\\U0001FAFF\"\n",
    "        \"\\U00002702-\\U000027B0\"\n",
    "        \"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\",\n",
    "        flags=re.UNICODE\n",
    "    )\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "df[\"text\"] = df[\"text\"].apply(remove_emojis)\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "test_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"], \n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "tokenized_dataset = test_dataset.map(tokenize_fn, batched=True)\n",
    "\n",
    "tokenized_dataset.set_format(\n",
    "    type=\"torch\",\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"label\"]\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(tokenized_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0583cfe8",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a79d6c8",
   "metadata": {},
   "source": [
    "This will run evaluation on test dataset as well as calculate the accuracy, precision, recall and f1 scores. (0 is clean, 1 is flagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c390c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        preds = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "    all_labels, all_preds, average=\"weighted\"\n",
    ")\n",
    "\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")\n",
    "\n",
    "precision_cls, recall_cls, f1_cls, support_cls = precision_recall_fscore_support(\n",
    "    all_labels, all_preds, labels=[0,1], average=None\n",
    ")\n",
    "\n",
    "print(\"Class-wise metrics:\")\n",
    "for i, (p, r, f, s) in enumerate(zip(precision_cls, recall_cls, f1_cls, support_cls)):\n",
    "    if i == 0:\n",
    "        print(f\"clean -> Precision: {p:.4f}, Recall: {r:.4f}, F1: {f:.4f}, Support: {s}\")\n",
    "    else:\n",
    "        print(f\"flagged -> Precision: {p:.4f}, Recall: {r:.4f}, F1: {f:.4f}, Support: {s}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "techjam2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
