{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7e8b7cc",
   "metadata": {},
   "source": [
    "# Initial Testing with pretrained LLM (Gemma, Qwen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1712e060",
   "metadata": {},
   "source": [
    "This is our initial testing with pretrained LLM. As the results are slow and expensive, we decided to switch to a better alternative with BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e2ba54",
   "metadata": {},
   "source": [
    "## Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad8d831",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install python-dotenv huggingface_hub pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc97c614",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12174ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from huggingface_hub import login, InferenceClient\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9426a1",
   "metadata": {},
   "source": [
    "## Login to HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b8c175",
   "metadata": {},
   "source": [
    "Place your huggingface token in `.env` file as mentioned in `README.md`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2822c0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f692849",
   "metadata": {},
   "source": [
    "## Load dataset (kaggle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e374569b",
   "metadata": {},
   "source": [
    "This dataset is from kaggle and used for testing llm solution. Download the dataset as mentioned in `README.md` and place in `./data` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4beb1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/reviews.csv\")\n",
    "reviews = df[\"text\"].dropna().tolist()\n",
    "print(df.head())\n",
    "print(len(df))\n",
    "print(len(reviews))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d812d3",
   "metadata": {},
   "source": [
    "## Prompt design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef178df3",
   "metadata": {},
   "source": [
    "We used a simple prompt to classify into the 3 policy violations and clean classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78412b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [f\"\"\"\n",
    "Classify the following Google review into one category.\n",
    "Categories: [Advertisement, Irrelevant Content, Rant Without Visit, Clean]\n",
    "Respond with only the category name.\n",
    "\n",
    "Review: {review}\n",
    "Answer:\n",
    "\"\"\" for review in reviews]\n",
    "print(prompts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09a566e",
   "metadata": {},
   "source": [
    "## Inference pipeline (Qwen3 on cloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df01e7ca",
   "metadata": {},
   "source": [
    "Due to heavy inference on local computer, we decided to inference on cloud using huggingface inference client. Qwen3 is chosen as Gemma has some issues with the inference client and Qwen3 is a great state of the art LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13255094",
   "metadata": {},
   "source": [
    "### Zero shot inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4358a9d",
   "metadata": {},
   "source": [
    "Zero shot inference with Qwen3 takes quite a long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e98a61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = InferenceClient()\n",
    "results = [client.chat_completion(\n",
    "    model=\"Qwen/Qwen3-8B\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    ") for prompt in prompts]\n",
    "cleaned = []\n",
    "for out in results:\n",
    "    parts = re.split(r\"</think>\", out.choices[0].message.content, maxsplit=1, flags=re.DOTALL)\n",
    "    if len(parts) > 1:\n",
    "        cleaned.append(parts[1].strip())\n",
    "    else:\n",
    "        cleaned.append(out.choices[0].message.content.strip())\n",
    "print(cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918032a5",
   "metadata": {},
   "source": [
    "### Few shots inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d44e246",
   "metadata": {},
   "source": [
    "Few shots inference with Qwen3 also takes quite a long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460a8c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = InferenceClient()\n",
    "results = [client.chat_completion(\n",
    "    model=\"Qwen/Qwen3-8B\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"I work for them Barre Vt Location\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Advertisement\"},\n",
    "        {\"role\": \"user\", \"content\": \"You can review a lake? How does that work\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Irrelevant Content\"},\n",
    "        {\"role\": \"user\", \"content\": \"Didn't go here lol\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Rant Without Visit\"},\n",
    "        {\"role\": \"user\", \"content\": \"We went to Marmaris with my wife for a holiday. We chose this restaurant as a place for dinner based on the reviews and because we wanted juicy food. When we first went there was a serious queue. You proceed by taking the food you want in the form of an open buffet. Both vegetable dishes and meat dishes were plentiful. There was also dessert for those who wanted it. After you get what you want you pay at the cashier. They don't go through cards they work in cash. There was a lot of food variety. And the food prices were unbelievably cheap. We paid only 84 TL for all the meals here. It included buttermilk and bread. But unfortunately I can't say it's too clean as a place.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Clean\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    ") for prompt in prompts]\n",
    "cleaned = []\n",
    "for out in results:\n",
    "    parts = re.split(r\"</think>\", out.choices[0].message.content, maxsplit=1, flags=re.DOTALL)\n",
    "    if len(parts) > 1:\n",
    "        cleaned.append(parts[1].strip())\n",
    "    else:\n",
    "        cleaned.append(out.choices[0].message.content.strip())\n",
    "print(cleaned)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "techjam2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
