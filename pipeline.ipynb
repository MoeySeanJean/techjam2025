{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87dd6886",
   "metadata": {},
   "source": [
    "# Solution pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0712600",
   "metadata": {},
   "source": [
    "This solution pipeline contains a call to Google Translate to translate the review, then the translated text is classified into flagged or clean with our model. If the review is empty text, it will be flagged as discussed in `README.md`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3248d22",
   "metadata": {},
   "source": [
    "## Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6657f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers datasets pandas scikit-learn googletrans\n",
    "%pip install torch torchvision --index-url https://download.pytorch.org/whl/cu129"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ff4612",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "46998e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from googletrans import Translator\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b0c699",
   "metadata": {},
   "source": [
    "## Load trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d014ae9e",
   "metadata": {},
   "source": [
    "This is to load the trained model from training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ca4aa3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"./data/saved_model\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BertForSequenceClassification.from_pretrained(save_dir, local_files_only=True)\n",
    "model.to(device)\n",
    "tokenizer = BertTokenizer.from_pretrained(save_dir, local_files_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e22c5b",
   "metadata": {},
   "source": [
    "## Load testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc49aa23",
   "metadata": {},
   "source": [
    "For testing and evaluation of our model, we used 2 datasets. One is Kaggle dataset as mentioned in `README.md`. The other is scrapped and labelled manually from local (Singapore) Google Maps reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac5c691",
   "metadata": {},
   "source": [
    "### Kaggle data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2e76d9",
   "metadata": {},
   "source": [
    "This data is from kaggle (see `README.md`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0249e81b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "414710538d3845d6a9d09313ba68d7dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/reviews-labeled.csv\")\n",
    "df = df[['text', 'label']]\n",
    "df = df.dropna(subset=[\"text\"]).reset_index(drop=True)\n",
    "df['label'] = df['label'].apply(lambda x : 0 if x == \"clean\" else 1)\n",
    "\n",
    "test_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"], \n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "tokenized_dataset = test_dataset.map(tokenize_fn, batched=True)\n",
    "\n",
    "tokenized_dataset.set_format(\n",
    "    type=\"torch\",\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"label\"]\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(tokenized_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f148ed",
   "metadata": {},
   "source": [
    "### Local (Singapore) data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0107063",
   "metadata": {},
   "source": [
    "This data is manually scrapped and labelled from Google Maps directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce8abe4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 're' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[67]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     23\u001b[39m     emoji_pattern = re.compile(\n\u001b[32m     24\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m[\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     25\u001b[39m         \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\U0001F600\u001b[39;00m\u001b[33m-\u001b[39m\u001b[38;5;130;01m\\U0001F64F\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     37\u001b[39m         flags=re.UNICODE\n\u001b[32m     38\u001b[39m     )\n\u001b[32m     39\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m emoji_pattern.sub(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m, text)\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m df[\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremove_emojis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28mprint\u001b[39m(df.head())\n\u001b[32m     45\u001b[39m test_dataset = Dataset.from_pandas(df)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\.conda\\envs\\techjam2025\\Lib\\site-packages\\pandas\\core\\series.py:4935\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4800\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4801\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4802\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4807\u001b[39m     **kwargs,\n\u001b[32m   4808\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4809\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4810\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4811\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4926\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4927\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4928\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4929\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4930\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4931\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4932\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4933\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4934\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4935\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\.conda\\envs\\techjam2025\\Lib\\site-packages\\pandas\\core\\apply.py:1422\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1419\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1421\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1422\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\.conda\\envs\\techjam2025\\Lib\\site-packages\\pandas\\core\\apply.py:1502\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1496\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1497\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1498\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1499\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1500\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1501\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1502\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1503\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1504\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1506\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1507\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1508\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\.conda\\envs\\techjam2025\\Lib\\site-packages\\pandas\\core\\base.py:925\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    922\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    923\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\.conda\\envs\\techjam2025\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/lib.pyx:2999\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[67]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mremove_emojis\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mremove_emojis\u001b[39m(text):\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     emoji_pattern = \u001b[43mre\u001b[49m.compile(\n\u001b[32m     24\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m[\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     25\u001b[39m         \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\U0001F600\u001b[39;00m\u001b[33m-\u001b[39m\u001b[38;5;130;01m\\U0001F64F\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     26\u001b[39m         \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\U0001F300\u001b[39;00m\u001b[33m-\u001b[39m\u001b[38;5;130;01m\\U0001F5FF\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     27\u001b[39m         \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\U0001F680\u001b[39;00m\u001b[33m-\u001b[39m\u001b[38;5;130;01m\\U0001F6FF\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     28\u001b[39m         \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\U0001F700\u001b[39;00m\u001b[33m-\u001b[39m\u001b[38;5;130;01m\\U0001F77F\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     29\u001b[39m         \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\U0001F780\u001b[39;00m\u001b[33m-\u001b[39m\u001b[38;5;130;01m\\U0001F7FF\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     30\u001b[39m         \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\U0001F800\u001b[39;00m\u001b[33m-\u001b[39m\u001b[38;5;130;01m\\U0001F8FF\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     31\u001b[39m         \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\U0001F900\u001b[39;00m\u001b[33m-\u001b[39m\u001b[38;5;130;01m\\U0001F9FF\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     32\u001b[39m         \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\U0001FA00\u001b[39;00m\u001b[33m-\u001b[39m\u001b[38;5;130;01m\\U0001FA6F\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     33\u001b[39m         \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\U0001FA70\u001b[39;00m\u001b[33m-\u001b[39m\u001b[38;5;130;01m\\U0001FAFF\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     34\u001b[39m         \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\U00002702\u001b[39;00m\u001b[33m-\u001b[39m\u001b[38;5;130;01m\\U000027B0\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     35\u001b[39m         \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\U000024C2\u001b[39;00m\u001b[33m-\u001b[39m\u001b[38;5;130;01m\\U0001F251\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     36\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m]+\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     37\u001b[39m         flags=re.UNICODE\n\u001b[32m     38\u001b[39m     )\n\u001b[32m     39\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m emoji_pattern.sub(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m, text)\n",
      "\u001b[31mNameError\u001b[39m: name 're' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/test-data-labeled.csv\")\n",
    "df = df[['text', 'label']]\n",
    "df = df.dropna(subset=[\"text\"]).reset_index(drop=True)\n",
    "\n",
    "texts = df['text'].to_list()\n",
    "indices = []\n",
    "texts_to_translate = []\n",
    "translated_texts = texts.copy()\n",
    "async def translate_bulk():\n",
    "    async with Translator() as translator:\n",
    "        for index, text in enumerate(texts):\n",
    "            result = await translator.detect(text)\n",
    "            if result.lang != 'en':\n",
    "                indices.append(index)\n",
    "                texts_to_translate.append(text)\n",
    "        translations = await translator.translate(texts_to_translate)\n",
    "        for i, translation in zip(indices, translations):\n",
    "            translated_texts[i] = translation.text\n",
    "await translate_bulk()\n",
    "df['text'] = translated_texts\n",
    "\n",
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        \"\\U0001F600-\\U0001F64F\"\n",
    "        \"\\U0001F300-\\U0001F5FF\"\n",
    "        \"\\U0001F680-\\U0001F6FF\"\n",
    "        \"\\U0001F700-\\U0001F77F\"\n",
    "        \"\\U0001F780-\\U0001F7FF\"\n",
    "        \"\\U0001F800-\\U0001F8FF\"\n",
    "        \"\\U0001F900-\\U0001F9FF\"\n",
    "        \"\\U0001FA00-\\U0001FA6F\"\n",
    "        \"\\U0001FA70-\\U0001FAFF\"\n",
    "        \"\\U00002702-\\U000027B0\"\n",
    "        \"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\",\n",
    "        flags=re.UNICODE\n",
    "    )\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "df[\"text\"] = df[\"text\"].apply(remove_emojis)\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "test_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"], \n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "tokenized_dataset = test_dataset.map(tokenize_fn, batched=True)\n",
    "\n",
    "tokenized_dataset.set_format(\n",
    "    type=\"torch\",\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"label\"]\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(tokenized_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0583cfe8",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a79d6c8",
   "metadata": {},
   "source": [
    "This will run evaluation on test dataset as well as calculate the accuracy, precision, recall and f1 scores. (0 is clean, 1 is flagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "58c390c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6476\n",
      "Precision: 0.9742\n",
      "Recall:    0.6476\n",
      "F1 Score:  0.7662\n",
      "Class-wise metrics:\n",
      "clean -> Precision: 0.9949, Recall: 0.6430, F1: 0.7812, Support: 3622\n",
      "flagged -> Precision: 0.0507, Recall: 0.8519, F1: 0.0956, Support: 81\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        preds = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "    all_labels, all_preds, average=\"weighted\"\n",
    ")\n",
    "\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")\n",
    "\n",
    "precision_cls, recall_cls, f1_cls, support_cls = precision_recall_fscore_support(\n",
    "    all_labels, all_preds, labels=[0,1], average=None\n",
    ")\n",
    "\n",
    "print(\"Class-wise metrics:\")\n",
    "for i, (p, r, f, s) in enumerate(zip(precision_cls, recall_cls, f1_cls, support_cls)):\n",
    "    if i == 0:\n",
    "        print(f\"clean -> Precision: {p:.4f}, Recall: {r:.4f}, F1: {f:.4f}, Support: {s}\")\n",
    "    else:\n",
    "        print(f\"flagged -> Precision: {p:.4f}, Recall: {r:.4f}, F1: {f:.4f}, Support: {s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf26682b",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7c8f1c",
   "metadata": {},
   "source": [
    "Full pipeline for our solution. Place real-world reviews in `reviews` and run the cell. It will print the prediction of our model. (0 is clean, 1 is flagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2ca27c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: My Roblox account got hacked from this location\n",
      "Predicted label: flagged\n",
      "\n",
      "Review: I hear this is a top university I wanna go here\n",
      "Predicted label: flagged\n",
      "\n",
      "Review: Amazing place for students worldwide. Top notch facilities for everything you care about. Really interesting lot of students to hang around. You'll love this space, the Campus is attracted all over the city of Singapore. For vegetarians, it's a bit tricky to get the desired food. Amazing public transport and AQI less than 50.\n",
      "Predicted label: clean\n",
      "\n",
      "Review: It's so fucking delicious, I recommend their spicy chicken\n",
      "Predicted label: clean\n",
      "\n",
      "Review: This restaurant has the best fat rice throughout Malaysia\n",
      "Predicted label: clean\n",
      "\n",
      "Review: This restaurant offers the best cookies in Riyadh\n",
      "Predicted label: clean\n",
      "\n",
      "Review: \n",
      "Predicted label: flagged\n",
      "\n",
      "Review: \n",
      "Predicted label: flagged\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews = [\n",
    "    \"My Roblox account got hacked from this location\",\n",
    "    \"I hear this is a top university I wanna go here\",\n",
    "    \"Amazing place for students worldwide. Top notch facilities for everything you care about. Really interesting lot of students to hang around. You'll love this space, the Campus is attracted all over the city of Singapore. For vegetarians, it's a bit tricky to get the desired food. Amazing public transport and AQI less than 50.\",\n",
    "    \"Áúü‰ªñÂ¶àÁöÑÂ•ΩÂêÉÔºåÊé®Ëçê‰ªñ‰ª¨ÁöÑËæ£Â≠êÈ∏°\",\n",
    "    \"restoran ini ada nasi lemak yang terbaik di seluruh Malaysia\",\n",
    "    \"Ÿáÿ∞ÿß ÿßŸÑŸÖÿ∑ÿπŸÖ ŸäŸÇÿØŸÖ ÿ£ŸÅÿ∂ŸÑ ŸÉÿ®ÿ≥ÿ© ŸÅŸä ÿßŸÑÿ±Ÿäÿßÿ∂\",\n",
    "    \"üëç\",\n",
    "    \"\"\n",
    "]\n",
    "df = pd.DataFrame(reviews, columns=['text'])\n",
    "\n",
    "texts = df['text'].to_list()\n",
    "indices = []\n",
    "texts_to_translate = []\n",
    "translated_texts = texts.copy()\n",
    "async def translate_bulk():\n",
    "    async with Translator() as translator:\n",
    "        for index, text in enumerate(texts):\n",
    "            result = await translator.detect(text)\n",
    "            if result.lang != 'en':\n",
    "                indices.append(index)\n",
    "                texts_to_translate.append(text)\n",
    "        translations = await translator.translate(texts_to_translate)\n",
    "        for i, translation in zip(indices, translations):\n",
    "            translated_texts[i] = translation.text\n",
    "await translate_bulk()\n",
    "df['text'] = translated_texts\n",
    "\n",
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        \"\\U0001F600-\\U0001F64F\"\n",
    "        \"\\U0001F300-\\U0001F5FF\"\n",
    "        \"\\U0001F680-\\U0001F6FF\"\n",
    "        \"\\U0001F700-\\U0001F77F\"\n",
    "        \"\\U0001F780-\\U0001F7FF\"\n",
    "        \"\\U0001F800-\\U0001F8FF\"\n",
    "        \"\\U0001F900-\\U0001F9FF\"\n",
    "        \"\\U0001FA00-\\U0001FA6F\"\n",
    "        \"\\U0001FA70-\\U0001FAFF\"\n",
    "        \"\\U00002702-\\U000027B0\"\n",
    "        \"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\",\n",
    "        flags=re.UNICODE\n",
    "    )\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "df[\"text\"] = df[\"text\"].apply(remove_emojis)\n",
    "\n",
    "results = [\"\"] * len(df)\n",
    "non_empty_indices = [i for i, r in enumerate(df['text']) if r.strip()]\n",
    "non_empty_texts = [df['text'][i] for i in non_empty_indices]\n",
    "if non_empty_texts:\n",
    "    inputs = tokenizer(non_empty_texts, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "    for idx, pred in zip(non_empty_indices, predictions):\n",
    "        results[idx] = \"clean\" if pred.item() == 0 else \"flagged\"\n",
    "for i, r in enumerate(df['text']):\n",
    "    if not r.strip():\n",
    "        results[i] = \"flagged\"\n",
    "df[\"label\"] = results\n",
    "for review, label in zip(df['text'], df['label']):\n",
    "    print(f\"Review: {review}\\nPredicted label: {label}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "techjam2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
