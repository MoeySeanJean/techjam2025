{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e03c951",
   "metadata": {},
   "source": [
    "# Solution pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b6e114",
   "metadata": {},
   "source": [
    "This solution pipeline contains a call to Google Translate to translate the review, then the translated text is classified into flagged or clean with our model. If the review is empty text, it will be flagged as discussed in `README.md`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a43db6",
   "metadata": {},
   "source": [
    "## Install packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d91a8da",
   "metadata": {},
   "source": [
    "Download torch according to official website and CUDA version as mentioned in `README.md`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892f6256",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers pandas googletrans\n",
    "%pip install torch torchvision --index-url https://download.pytorch.org/whl/cu129"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a8ef5a",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864ad35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch\n",
    "import pandas as pd\n",
    "from googletrans import Translator\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c540f08",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fa1d43",
   "metadata": {},
   "source": [
    "This will load the trained model from our training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0a8916",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"./data/saved_model\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BertForSequenceClassification.from_pretrained(save_dir, local_files_only=True)\n",
    "model.to(device)\n",
    "tokenizer = BertTokenizer.from_pretrained(save_dir, local_files_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424a1ea4",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9cdd3e",
   "metadata": {},
   "source": [
    "Full pipeline for our solution. Place real-world reviews in `reviews` and run the cell. It will print the prediction of our model (0 is clean, 1 is flagged). The predictions will be saved into a csv file `predictions.csv` in `./data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ec43b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = [\n",
    "    \"My Roblox account got hacked from this location\",\n",
    "    \"I hear this is a top university I wanna go here\",\n",
    "    \"Amazing place for students worldwide. Top notch facilities for everything you care about. Really interesting lot of students to hang around. You'll love this space, the Campus is attracted all over the city of Singapore. For vegetarians, it's a bit tricky to get the desired food. Amazing public transport and AQI less than 50.\",\n",
    "    \"Áúü‰ªñÂ¶àÁöÑÂ•ΩÂêÉÔºåÊé®Ëçê‰ªñ‰ª¨ÁöÑËæ£Â≠êÈ∏°\",\n",
    "    \"restoran ini ada nasi lemak yang terbaik di seluruh Malaysia\",\n",
    "    \"Ÿáÿ∞ÿß ÿßŸÑŸÖÿ∑ÿπŸÖ ŸäŸÇÿØŸÖ ÿ£ŸÅÿ∂ŸÑ ŸÉÿ®ÿ≥ÿ© ŸÅŸä ÿßŸÑÿ±Ÿäÿßÿ∂\",\n",
    "    \"üëç\",\n",
    "    \"\"\n",
    "]\n",
    "df = pd.DataFrame(reviews, columns=['original_text'])\n",
    "\n",
    "texts = df['original_text'].to_list()\n",
    "indices = []\n",
    "texts_to_translate = []\n",
    "translated_texts = texts.copy()\n",
    "async def translate_bulk():\n",
    "    async with Translator() as translator:\n",
    "        for index, text in enumerate(texts):\n",
    "            result = await translator.detect(text)\n",
    "            if result.lang != 'en':\n",
    "                indices.append(index)\n",
    "                texts_to_translate.append(text)\n",
    "        translations = await translator.translate(texts_to_translate)\n",
    "        for i, translation in zip(indices, translations):\n",
    "            translated_texts[i] = translation.text\n",
    "await translate_bulk()\n",
    "df['text'] = translated_texts\n",
    "\n",
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        \"\\U0001F600-\\U0001F64F\"\n",
    "        \"\\U0001F300-\\U0001F5FF\"\n",
    "        \"\\U0001F680-\\U0001F6FF\"\n",
    "        \"\\U0001F700-\\U0001F77F\"\n",
    "        \"\\U0001F780-\\U0001F7FF\"\n",
    "        \"\\U0001F800-\\U0001F8FF\"\n",
    "        \"\\U0001F900-\\U0001F9FF\"\n",
    "        \"\\U0001FA00-\\U0001FA6F\"\n",
    "        \"\\U0001FA70-\\U0001FAFF\"\n",
    "        \"\\U00002702-\\U000027B0\"\n",
    "        \"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\",\n",
    "        flags=re.UNICODE\n",
    "    )\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "df[\"text\"] = df[\"text\"].apply(remove_emojis)\n",
    "\n",
    "results = [\"\"] * len(df)\n",
    "non_empty_indices = [i for i, r in enumerate(df['text']) if r.strip()]\n",
    "non_empty_texts = [df['text'][i] for i in non_empty_indices]\n",
    "if non_empty_texts:\n",
    "    inputs = tokenizer(non_empty_texts, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "    for idx, pred in zip(non_empty_indices, predictions):\n",
    "        results[idx] = \"clean\" if pred.item() == 0 else \"flagged\"\n",
    "for i, r in enumerate(df['text']):\n",
    "    if not r.strip():\n",
    "        results[i] = \"flagged\"\n",
    "df[\"label\"] = results\n",
    "for review, label in zip(df['text'], df['label']):\n",
    "    print(f\"Review: {review}\\nPredicted label: {label}\\n\")\n",
    "\n",
    "df = df[['original_text', 'label']]\n",
    "df.to_csv(\"./data/predictions.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "techjam2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
